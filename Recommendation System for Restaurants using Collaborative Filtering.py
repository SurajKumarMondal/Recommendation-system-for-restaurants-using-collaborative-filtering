# -*- coding: utf-8 -*-
"""Univ.AI_Restaurant_Recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17n_0EAZxfQF8zjdCJPdEB5kTr9fblylQ
"""

#@title Import Libraries { vertical-output: true, display-mode: "form" }

#Using LighFM for only Train test splitting
!pip install lightfm

import pandas as pd
import numpy as np
import ast
import gc
from scipy import sparse
import sklearn
sklearn.__version__
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import scale, StandardScaler
from lightfm.cross_validation import random_train_test_split
from math import sqrt
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import warnings
from sklearn.decomposition import PCA
import seaborn as sns
from tqdm import tqdm
from prettytable import PrettyTable
warnings.filterwarnings('ignore')
from wordcloud import WordCloud
from collections import Counter

"""# Load Dataset

### Load and filter business dataframe
"""

#@title Load business dataset { display-mode: "form" }

business_df = pd.read_json('/content/drive/MyDrive/yelp_restaurant_recommendation/yelp_academic_dataset_business.json', lines=True)
business_df.shape

business_df.head(2)

"""Lets see what the 25 top businesses are on Yelp"""

#@title Top 25 business on Yelp { vertical-output: true, display-mode: "form" }
fig, ax = plt.subplots(figsize=[5,10])
sns.countplot(data=business_df[business_df['categories'].isin( business_df['categories'].value_counts().head(25).index)],y='categories', ax=ax)
plt.title('Top 25 business on Yelp')
plt.show()

"""Filter only Restaurants from business categories"""

business_df = business_df.drop('hours', axis=1)
business_df = business_df.dropna(axis = 0, how = 'any')
business_df = business_df[business_df['categories'].str.contains('Restaurants')]
business_df.shape

business_df.head(2)

#@title Top 10 cities by number of businesses listed { vertical-output: true, display-mode: "form" }
city_business_counts = business_df[['city', 'business_id']].groupby(['city'])['business_id'].agg('count').sort_values(ascending=False)
city_business_counts = pd.DataFrame(data=city_business_counts)
city_business_counts.rename(columns={'business_id' : 'number_of_businesses'}, inplace=True)
city_business_counts[0:10].sort_values(ascending=True, by="number_of_businesses").plot(kind='bar', stacked=False, figsize=[10,5], colormap='winter')
plt.title('Top 10 cities by businesses listed')
plt.show()

#@title Top 20 cities by reviews { vertical-output: true, display-mode: "form" }
city_business_reviews = business_df[['city', 'review_count', 'stars']].groupby(['city']).agg({'review_count': 'sum', 'stars': 'mean'}).sort_values(by='review_count', ascending=False)
city_business_reviews['review_count'][0:20].plot(kind='bar', stacked=False, figsize=[10,5], colormap='winter')
plt.title('Top 20 cities by reviews')
plt.show()

"""We won't be recommending restaurants that are no longer open, hence we drop restaurants that are closed."""

business_df = business_df[business_df.is_open==1]
business_df.shape

#@title Top 10 most reviewed restaurants { vertical-output: true, display-mode: "form" }
business_df[['name', 'review_count', 'city', 'stars']].sort_values(ascending=False, by="review_count")[0:10]
business_df['name'].value_counts().sort_values(ascending=False).head(10).plot(kind='pie',figsize=(10,6), 
title="Top 10 Most Reviewed Restaurants", autopct='%1.2f%%')
plt.axis('equal')
plt.show()

#@title Distribution of Restaurant Rating { vertical-output: true, display-mode: "form" }
plt.figure(figsize=(12,4))
ax = sns.countplot(business_df['stars'])
plt.title('Distribution of Restaurant Rating');

"""#### Extracting features for restaurants"""

restaurant_categories_dummies = pd.Series(business_df['categories']).str.get_dummies(',')
restaurant_categories_dummies.head()

"""Here we see there are 1040 features, but some of them are unrelated to restaurants such as Accountants, Accessories, Air Duct cleaning. Lets look for ways to remove them"""

#Dropping Columns that are 99% values as zero
restaurant_categories_dummies = restaurant_categories_dummies.loc[:, (restaurant_categories_dummies==0).mean()<0.99]
restaurant_categories_dummies.head()

"""We see the columns are reduced from 1040 to 83, and the columns are related to food/restaurant businesses"""

def extract_keys(attr, key):
    if attr == None:
        return "{}"
    if key in attr:
        return attr.pop(key)

def str_to_dict(attr):
    if attr != None:
        return ast.literal_eval(attr)
    else:
        return ast.literal_eval("{}")

"""Vectorizing attributes column"""

attributes = business_df[['attributes']]
attributes['BusinessParking'] = attributes.apply(lambda x: str_to_dict(extract_keys(x['attributes'], 'BusinessParking')), axis=1)
attributes['Ambience'] = attributes.apply(lambda x: str_to_dict(extract_keys(x['attributes'], 'Ambience')), axis=1)
attributes['GoodForMeal'] = attributes.apply(lambda x: str_to_dict(extract_keys(x['attributes'], 'GoodForMeal')), axis=1)
attributes['Dietary'] = attributes.apply(lambda x: str_to_dict(extract_keys(x['attributes'], 'Dietary')), axis=1)
attributes['Music'] = attributes.apply(lambda x: str_to_dict(extract_keys(x['attributes'], 'Music')), axis=1)
attributes.head()

df_attr = pd.concat([ attributes['attributes'].apply(pd.Series), attributes['BusinessParking'].apply(pd.Series),
                    attributes['Ambience'].apply(pd.Series), attributes['GoodForMeal'].apply(pd.Series), 
                    attributes['Dietary'].apply(pd.Series) ], axis=1)
df_attr_dummies = pd.get_dummies(df_attr, drop_first=True)
df_attr_dummies.head()

#Dropping Columns that are 99% values as zero
df_attr_dummies = df_attr_dummies.loc[:, (df_attr_dummies==0).mean()<0.99]
df_attr_dummies.head()

# restaurant_features = pd.concat([restaurant_categories_dummies, df_attr_dummies], axis=1)
# restaurant_features.drop('Restaurants',inplace=True,axis=1)
# restaurant_features.head()

"""This is the final vectorized restaurant dataframe"""

restaurant_df = pd.concat([business_df[['business_id', 'name', 'stars']], restaurant_categories_dummies, df_attr_dummies], axis=1)
restaurant_df.head()

restaurant_df.shape

del restaurant_categories_dummies
del df_attr_dummies
del attributes

gc.collect()

"""### Load User DataFrame"""

user_df = pd.DataFrame()
for chunk in pd.read_json('/content/drive/MyDrive/yelp_restaurant_recommendation/yelp_academic_dataset_user.json', chunksize=100000, lines=True):
  # chunk = chunk[chunk['business_id'].isin(business_df['business_id'])]
  user_df = pd.concat([user_df, chunk])
user_df.shape

"""In order to reduce the number of users, we will select only users who have more than 10 posts tagged as useful are the top 100 most active users"""

# user_df = user_df[(user_df.useful>10)&(user_df.review_count>200)]
user_df = user_df[(user_df.useful>10)].sort_values(by=['review_count'], ascending=False).head(100)
user_df.shape

user_df.head()

"""### Load checkin dataset"""

checkin_df = pd.read_json('/content/drive/MyDrive/yelp_restaurant_recommendation/yelp_academic_dataset_checkin.json', lines=True)
checkin_df.head(2)

checkin_df.shape

"""### Load Tip Dataset"""

tip_df = pd.read_json('/content/drive/MyDrive/yelp_restaurant_recommendation/yelp_academic_dataset_tip.json', lines=True)
tip_df.head(2)

"""## Load Review dataset"""

#Loading in chunks due to OOM error
review_df = pd.DataFrame()
for chunk in pd.read_json('/content/drive/MyDrive/yelp_restaurant_recommendation/yelp_academic_dataset_review.json', chunksize=100000, lines=True):
  chunk = chunk[(chunk['business_id'].isin(restaurant_df['business_id']))&(chunk['user_id'].isin(user_df['user_id']))]
  review_df = pd.concat([review_df, chunk])

review_df.head()

review_df.shape

"""# Create User-Item matrix"""

#Create mappers
user_id_map = {id:data for id, data in enumerate(review_df.user_id.unique())}
restaurant_id_map = {id:data for id, data in enumerate(review_df.business_id.unique())}

inv_user_id_map = {id:idx for idx, id in user_id_map.items()}
inv_restaurant_id_map = {id:idx for idx, id in restaurant_id_map.items()}

review_df['business_idx'] = review_df['business_id'].map(inv_restaurant_id_map)
review_df['user_idx'] = review_df['user_id'].map(inv_user_id_map)
review_df.head()

review_df.isna().sum()

review_df['stars'] = review_df.stars.astype('int32')
review_df['user_idx'] = review_df.user_idx.astype('int32')
review_df['business_idx'] = review_df.business_idx.astype('int32')

review_df.dtypes

n_users = len(user_id_map)
n_items = len(restaurant_id_map)
print(n_users, n_items)

review_df.head(2)

ratings = review_df.pivot_table(values='stars', index ='user_idx', columns='business_idx')
ratings = sparse.csr_matrix(ratings.fillna(0).values)
ratings.shape

sparsity = ratings.getnnz()
sparsity /= (n_items * n_users)
sparsity *= 100
print (f'Sparsity: {sparsity}')

"""# Create Similarity matrix
For similarity, we are using Cosine similarity
"""

item_similarity = cosine_similarity(ratings.transpose())
item_similarity.shape

def k_similar_restaurants(similarity, restaurant_idx, mapper=restaurant_id_map, k=6):
    # similarity = np.argsort(similarity[restaurant_idx,:])[:-k-1:-1]
    return [mapper[x] for x in np.argsort(similarity[restaurant_idx,:])[:-k-1:-1]], np.sort(similarity[restaurant_idx,:])[:-k-1:-1]
  
def show_similar_restaurants(dataframe, similarity, restaurant_idx, mapper=restaurant_id_map, k=6):
    id, sim = k_similar_restaurants(similarity, restaurant_idx, mapper, k)
    new_df = dataframe[dataframe.business_id.isin(id)][['business_id', 'name', 'attributes','categories']].set_index('business_id').reindex(id)
    # new_df['similarity'] = sim
    return new_df

#@title Get restaurants similar to restaurant_idx { display-mode: "form" }
restaurant_idx = 99#@param {type:"number"}
show_similar_restaurants(business_df, item_similarity, restaurant_idx = 99)

"""The above dataframe shows similar restaurants to idx=99 ie Burger King (first row of the dataframe). We can aproximate this similarity from the columns 'attributes' and 'categories'. We see that the restaurants recommends fast-food places in categories such as  pizzas, burgers and donuts and with similar attributes such as noiseLevel:average and RestaurantTakeOut as True

# Build a CF-based Recommender system using Stochastic Gradient Descent

## Create train-test dataset
"""

train, val = random_train_test_split(ratings, test_percentage=0.2, random_state=24)

train = train.tocsr()
val = val.tocsr()

def rmse(prediction, ground_truth):
    prediction = prediction[ground_truth.nonzero()].reshape(-1,1)
    ground_truth = ground_truth[ground_truth.nonzero()].reshape(-1,1)
    # print(prediction.shape, ground_truth.shape)
    return sqrt(mean_squared_error(prediction, ground_truth))

"""## Build Recommender System

### Recommendation System using randomly initialized latent vectors
Here we are randomly initializing the latent features and train them by stochastic gradient descent
"""

class Recommender:
  
  def __init__(self, n_epochs=200, n_latent_features=3, lmbda=0.1, learning_rate=0.001):
    self.n_epochs = n_epochs
    self.n_latent_features = n_latent_features
    self.lmbda = lmbda
    self.learning_rate = learning_rate
  
  def predictions(self, P, Q):
    return np.dot(P.T, Q)
  
  def fit(self, X_train, X_val):
    m, n = X_train.shape

    self.P = 3 * np.random.rand(self.n_latent_features, m)
    self.Q = 3 * np.random.rand(self.n_latent_features, n)
    
    self.train_error = []
    self.val_error = []

    users, items = X_train.nonzero()
    
    for epoch in tqdm(range(self.n_epochs)):
        for u, i in zip(users, items):
            error = X_train[u, i] - self.predictions(self.P[:,u], self.Q[:,i])
            self.P[:, u] += self.learning_rate * (error * self.Q[:, i] - self.lmbda * self.P[:, u])
            self.Q[:, i] += self.learning_rate * (error * self.P[:, u] - self.lmbda * self.Q[:, i])

        train_rmse = rmse(self.predictions(self.P, self.Q), X_train)
        val_rmse = rmse(self.predictions(self.P, self.Q), X_val)

        self.train_error.append(train_rmse)
        self.val_error.append(val_rmse)
        
    return self
  
  def predict(self, X_train, user_index):
    y_hat = self.predictions(self.P, self.Q)
    predictions_index = sparse.find(X_train[user_index, :] == 0)[1]
    return predictions_index, np.clip(y_hat[user_index, predictions_index].flatten(),0,5)

recommender = Recommender(n_epochs=200, n_latent_features=5, lmbda=0.1, learning_rate=0.0002).fit(train, val)

plt.plot(range(recommender.n_epochs), recommender.train_error, marker='o', label='Training Data');
plt.plot(range(recommender.n_epochs), recommender.val_error, marker='v', label='Validation Data');
plt.xlabel('Number of Epochs');
plt.ylabel('RMSE');
plt.legend()
plt.grid()
plt.title("Recommendation System using randomly initialized latent vectors")
plt.show()

print(f"The RMSE is {str(round(recommender.val_error[-1],2))} at {str(recommender.n_epochs)} epoch")

"""#### Making recommendations"""

#@title Enter User index to make recommendations. { display-mode: "form" }

user_index = 10 #@param {type:"number"}
number_of_recommendations =  5#@param {type:"number"}
def display_user_restaurants(indexes, ratings, dataframe, mapper=restaurant_id_map, k=10):
    max_ratings_idx = np.argsort(ratings)[-k:]
    id = np.vectorize(mapper.get)(indexes[max_ratings_idx])
    new_df = business_df[business_df.business_id.isin(id)][['business_id', 'name', 'attributes','categories']].set_index('business_id').reindex(id)
    # new_df['ratings'] = ratings[max_ratings_idx]
    return new_df[[ 'name','attributes','categories']]

predicted_idx, predicted_ratings = recommender.predict(train, user_index)
display_user_restaurants(predicted_idx, predicted_ratings, business_df, k=number_of_recommendations)

#@title Top k restaurants the user rated. { display-mode: "form" }
k = 5#@param {type:"number"}
existing_ratings_index = train[user_index, :].nonzero()[1]
existing_ratings = train[user_index, existing_ratings_index].toarray().flatten()
display_user_restaurants(existing_ratings_index, existing_ratings, business_df, k=k)

"""### Recommendation System using Restaurant features
In the previous recommendation system, we randomly initialized the latent features. In this recommendation system, we will use a static restaurant feature and use gradient descent to create user embeddings.
"""

restaurant_df = restaurant_df[restaurant_df.business_id.isin(inv_restaurant_id_map.keys())]
restaurant_df.head()

restaurant_df['business_idx']=restaurant_df.business_id.map(inv_restaurant_id_map)
restaurant_df = restaurant_df.set_index('business_idx').sort_index()
restaurant_df.head()

restaurant_features = restaurant_df.drop(['business_id','name','stars'], axis=1)
restaurant_features.head()

"""Create custom recommender for static restaurant features
Here, with every epoch, the restaurant features will not be updated with gradient descent
"""

class Custom_Recommender:
  
  def __init__(self, item_features, n_epochs=200, lmbda=0.1, learning_rate=0.001):
    self.n_epochs = n_epochs
    self.Q = item_features.T
    self.n_latent_features = self.Q.shape[0]
    self.lmbda = lmbda
    self.learning_rate = learning_rate
  
  def predictions(self, P, Q):
    return np.dot(P.T, Q)
  
  def fit(self, X_train, X_val):
    m, n = X_train.shape

    self.P = 3 * np.random.rand(self.n_latent_features, m)
    # self.Q = 3 * np.random.rand(self.n_latent_features, n)
    
    self.train_error = []
    self.val_error = []

    users, items = X_train.nonzero()
    
    for epoch in tqdm(range(self.n_epochs)):
        for u, i in zip(users, items):
            error = X_train[u, i] - self.predictions(self.P[:,u], self.Q[:,i])
            self.P[:, u] += self.learning_rate * (error * self.Q[:, i] - self.lmbda * self.P[:, u])
            # self.Q[:, i] += self.learning_rate * (error * self.P[:, u] - self.lmbda * self.Q[:, i])

        train_rmse = rmse(self.predictions(self.P, self.Q), X_train)
        val_rmse = rmse(self.predictions(self.P, self.Q), X_val)

        self.train_error.append(train_rmse)
        self.val_error.append(val_rmse)
        
    return self
  
  def predict(self, X_train, user_index):
    y_hat = self.predictions(self.P, self.Q)
    predictions_index = sparse.find(X_train[user_index, :] == 0)[1]
    return predictions_index, np.clip(y_hat[user_index, predictions_index].flatten(),0,5)

recommender_1 = Custom_Recommender(n_epochs=500, item_features =restaurant_features.values, lmbda=0.1, learning_rate=0.0001).fit(train, val)

plt.plot(range(recommender_1.n_epochs), recommender_1.train_error, marker='o', label='Training Data');
plt.plot(range(recommender_1.n_epochs), recommender_1.val_error, marker='v', label='Validation Data');
plt.xlabel('Number of Epochs');
plt.ylabel('RMSE');
plt.legend()
plt.grid()
plt.title("Recommendation System using Restaurant Features")
plt.show()

print(f"The min RMSE is {str(round(recommender_1.val_error[-1],2))} at {str(recommender_1.n_epochs)} epoch")

"""### Using PCA on restaurant features
The restaurants have 141 features which would create a large sparse embedding, hence we will reduce it by using PCA on the features
"""

pca = PCA(n_components=5, random_state=24)
pca_features = pca.fit_transform(restaurant_features)

recommender_2 = Custom_Recommender(n_epochs=500, item_features =pca_features, lmbda=0.1, learning_rate=0.0001).fit(train, val)

plt.plot(range(recommender_2.n_epochs), recommender_2.train_error, marker='o', label='Training Data');
plt.plot(range(recommender_2.n_epochs), recommender_2.val_error, marker='v', label='Validation Data');
plt.xlabel('Number of Epochs');
plt.ylabel('RMSE');
plt.legend()
plt.title("Recommendation System with PCA on restaurant features")
plt.grid()
plt.show()

print(f"The min RMSE is {str(round(recommender_2.val_error[-1],2))} at {str(recommender_2.n_epochs)} epoch")

"""#### Making recommendations"""

#@title Enter User index to make recommendations. { display-mode: "form" }

user_index = 10 #@param {type:"number"}
number_of_recommendations =  5#@param {type:"number"}
def display_user_restaurants(indexes, ratings, dataframe, mapper=restaurant_id_map, k=10):
    max_ratings_idx = np.argsort(ratings)[-k:]
    id = np.vectorize(mapper.get)(indexes[max_ratings_idx])
    new_df = business_df[business_df.business_id.isin(id)][['business_id', 'name', 'attributes','categories']].set_index('business_id').reindex(id)
    # new_df['ratings'] = ratings[max_ratings_idx]
    return new_df[[ 'name','attributes','categories']]

predicted_idx, predicted_ratings = recommender_2.predict(train, user_index)
display_user_restaurants(predicted_idx, predicted_ratings, business_df, k=number_of_recommendations)

#@title Top k restaurants the user rated. { display-mode: "form" }
k = 5#@param {type:"number"}
existing_ratings_index = train[user_index, :].nonzero()[1]
existing_ratings = train[user_index, existing_ratings_index].toarray().flatten()
display_user_restaurants(existing_ratings_index, existing_ratings, business_df, k=k)

"""# Comparison of CF-based recommender system"""

x = PrettyTable()
x.field_names = ["Sr. no.","CF Recommender type","RMSE"]
x.add_row([1, 'CF Recommender using randomly initialized latent vectors',round(recommender.val_error[-1],2)])
x.add_row([2, 'CF Recommender using restaurant features',round(recommender_1.val_error[-1],2)])
x.add_row([3, 'CF Recommender with PCA on restaurant features',round(recommender_2.val_error[-1],2)])
print(x)

"""# Further Analysis"""

#@title 10 most popular restaurant categories in the city { vertical-output: true, display-mode: "form" }
city = 'Edmonton'#@param {type:"string"}
plt.figure(figsize=(15,5))

top_25_business = business_df[business_df.city==city].sort_values(['stars', 'review_count'], ascending=[False, False]).head(25)
categories = []
for i in top_25_business.categories:
  categories.extend(j.strip() for j in i.split(','))
counter = Counter(categories)
del counter['Restaurants']
del counter['Food']
counter = counter.most_common()[:10]
plt.bar([c for c, _ in counter], [c for  _, c in counter])
plt.title(f"10 most popular restaurant categories in {city}")
plt.xlabel("Restaurant Category")
plt.xticks(rotation=90)
plt.ylabel("Frequency")
plt.show()

"""Try Cities 'Philadelphia' and 'Edmonton'.

We observe the differences in the popular restaurant categories of the area. In Philadelphia sandwiches, mexican and italian foods are more popular whereas in Edmonton Indian food, cafes and canadian food are more popular. Thus the popularity of a new restaurant in a city can be estimated by the category of the restaurant 
"""